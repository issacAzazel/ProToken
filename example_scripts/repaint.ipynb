{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Zero-shot co-engineering of protein sequences & structrues with PT-DiT"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook provides examples of utilizing RePaint ([arXiv:2201.09865v4](https://arxiv.org/abs/2201.09865)) algorithm with PT-DiT, a pre-trained multimodal diffusion model, to co-engineer protein sequences (represented as amino acids) and structures (represented as ProTokens)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"XLA_PYTHON_CLIENT_PREALLOCATE\"] = \"False\"\n",
    "import sys\n",
    "sys.path.append('..')\n",
    "\n",
    "import jax\n",
    "import jax.numpy as jnp\n",
    "import pickle as pkl\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import argparse\n",
    "\n",
    "from functools import partial\n",
    "from src.model.diffusion_transformer import DiffusionTransformer\n",
    "from train.schedulers import GaussianDiffusion\n",
    "import datetime\n",
    "from flax.jax_utils import replicate\n",
    "from functools import reduce\n",
    "\n",
    "from configs.global_config import global_config\n",
    "from configs.dit_config import dit_config\n",
    "global_config.dropout_flag = False \n",
    "\n",
    "### Load embedding \n",
    "with open('../embeddings/protoken_emb.pkl', 'rb') as f:\n",
    "    protoken_emb = jnp.array(pkl.load(f), dtype=jnp.float32)\n",
    "with open('../embeddings/aatype_emb.pkl', 'rb') as f:\n",
    "    aatype_emb = jnp.array(pkl.load(f), dtype=jnp.float32)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Preparation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 Define Constants"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we define constants for PT-DiT inference:\n",
    "* `NRES`: length of proteins \n",
    "* `NSAMPLE_PER_DEVICE`: the number of proteins in each GPU device\n",
    "* `DIM_EMB`: the size of raw embedding (concatenating ProTokens & amino acids)\n",
    "* `NDEVICES`: the number of available GPU devices\n",
    "* `BATCH_SIZE`: the number of proteins generated in each batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### constants\n",
    "NRES = 512\n",
    "NSAMPLE_PER_DEVICE = 8\n",
    "DIM_EMB_PTK = protoken_emb.shape[-1]\n",
    "DIM_EMB_AA = aatype_emb.shape[-1]\n",
    "DIM_EMB = DIM_EMB_PTK + DIM_EMB_AA # 40 # 32 + 8\n",
    "NDEVICES = len(jax.devices())\n",
    "\n",
    "BATCH_SIZE = NSAMPLE_PER_DEVICE * NDEVICES"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 Define Functional Utils for Pre/Post-processing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we define functional utils for pre-processing inputs and post-processing outputs, for example:\n",
    "* `protoken_emb_distance_fn`: calculate Euclidean distance between two ProToken embeddings \n",
    "* `aatype_emb_distance_fn`: calculate Euclidean distance between two amino acid embeddings\n",
    "* `aatype_index_to_resname`: convert amino acid indexes into corresponding amino acid symbols\n",
    "* `resname_to_aatype_index`: convert amino acid symbols into corresponding amino acid indexes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### function utils \n",
    "\n",
    "def split_multiple_rng_keys(rng_key, num_keys):\n",
    "    rng_keys = jax.random.split(rng_key, num_keys + 1)\n",
    "    return rng_keys[:-1], rng_keys[-1]\n",
    "\n",
    "def flatten_list_of_dicts(list_of_dicts):\n",
    "    ### [{a: [1,2,3,4]}] -> [{a:1}, {a:2}, {a:3}, {a:4}]\n",
    "    flattened_lists = [[{k: v[i] for k, v in d.items()} \n",
    "                        for i in range(len(next(iter(d.values()))))] for d in list_of_dicts]\n",
    "    return reduce(lambda x, y: x+y, flattened_lists, [])\n",
    "\n",
    "def protoken_emb_distance_fn(x, y):\n",
    "    x_ = x / (jnp.linalg.norm(x, axis=-1, keepdims=True) + 1e-6)\n",
    "    y_ = y / (jnp.linalg.norm(y, axis=-1, keepdims=True) + 1e-6)\n",
    "    \n",
    "    return -jnp.sum(x_ * y_, axis=-1)\n",
    "\n",
    "def aatype_emb_distance_fn(x, y):\n",
    "    return jnp.sum((x - y) ** 2, axis=-1)\n",
    "\n",
    "def aatype_index_to_resname(aatype_index):\n",
    "    restypes = [\n",
    "        'A', 'R', 'N', 'D', 'C', 'Q', 'E', 'G', 'H', 'I', 'L', 'K', 'M', 'F', 'P',\n",
    "        'S', 'T', 'W', 'Y', 'V'\n",
    "    ]\n",
    "    \n",
    "    return \"\".join([restypes[int(i)] for i in aatype_index])\n",
    "\n",
    "def resname_to_aatype_index(resnames):\n",
    "    restypes = [\n",
    "        'A', 'R', 'N', 'D', 'C', 'Q', 'E', 'G', 'H', 'I', 'L', 'K', 'M', 'F', 'P',\n",
    "        'S', 'T', 'W', 'Y', 'V'\n",
    "    ]\n",
    "    return np.array([restypes.index(a) for a in resnames], dtype=np.int32)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3 Load Model & Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### load model & params \n",
    "dit_model = DiffusionTransformer(\n",
    "    config=dit_config, global_config=global_config\n",
    ")\n",
    "num_diffusion_timesteps = 500\n",
    "scheduler = GaussianDiffusion(num_diffusion_timesteps=num_diffusion_timesteps)\n",
    "\n",
    "#### rng keys\n",
    "rng_key = jax.random.PRNGKey(8888)\n",
    "np.random.seed(7777)\n",
    "\n",
    "##### load params\n",
    "ckpt_path = '../ckpts/PT_DiT_params_2000000.pkl'\n",
    "with open(ckpt_path, \"rb\") as f:\n",
    "    params = pkl.load(f)\n",
    "    params = jax.tree_util.tree_map(lambda x: jnp.array(x), params)\n",
    "    \n",
    "##### replicate params\n",
    "params = replicate(params)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.4 Define Functional Utils for Inference"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we define functional utils for PT-DiT inference, for example:\n",
    "* `clamp_x0_fn`: to enable clamping trick introduced at [arXiv:2205.14217v1](https://arxiv.org/abs/2205.14217)\n",
    "* `denoise_step`: sample $p(\\mathbf{z}_{t-1}|\\mathbf{z}_t)$\n",
    "* `q_sample`: sample $p(\\mathbf{z}_t|\\mathbf{z}_0)$\n",
    "* `noise_step`: sample $p(\\mathbf{z}_t | \\mathbf{z}_{t-1})$\n",
    "\n",
    "and define `pjit = pmap + jit` (see `jax` documentations) version of functions to enable fast and batched inference. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##### main inference functions\n",
    "jit_apply_fn = jax.jit(dit_model.apply)\n",
    "infer_protuple = True\n",
    "\n",
    "def clamp_x0_fn(x0):\n",
    "    protoken_indexes = \\\n",
    "                jnp.argmin(protoken_emb_distance_fn(x0[..., None, :protoken_emb.shape[-1]], \n",
    "                                                  protoken_emb.reshape((1,)*(len(x0.shape)-1) + protoken_emb.shape)), axis=-1)\n",
    "    if bool(infer_protuple):\n",
    "        aatype_indexes = \\\n",
    "                jnp.argmin(aatype_emb_distance_fn(x0[..., None, protoken_emb.shape[-1]:], \n",
    "                                                  aatype_emb.reshape((1,)*(len(x0.shape)-1) + aatype_emb.shape)), axis=-1)\n",
    "        return jnp.concatenate([protoken_emb[protoken_indexes], aatype_emb[aatype_indexes]], axis=-1)\n",
    "    else:\n",
    "        return protoken_emb[protoken_indexes]\n",
    "\n",
    "def denoise_step(params, x, seq_mask, t, residue_index, rng_key,\n",
    "                 clamp_x0_fn=None):\n",
    "    t = jnp.full((x.shape[0],), t)\n",
    "    indicator = params['params']['protoken_indicator']\n",
    "    if bool(infer_protuple):\n",
    "        indicator = jnp.concatenate([indicator, params['params']['aatype_indicator']], \n",
    "                                    axis=-1)\n",
    "    eps_prime = jit_apply_fn({'params': params['params']['model']}, x + indicator[None, ...], \n",
    "                             seq_mask, t, tokens_rope_index=residue_index)\n",
    "    mean, variance, log_variance = scheduler.p_mean_variance(x, t, eps_prime, clip=False, clamp_x0_fn=clamp_x0_fn)\n",
    "    rng_key, normal_key = jax.random.split(rng_key)\n",
    "    x = mean + jnp.exp(0.5 * log_variance) * jax.random.normal(normal_key, x.shape)\n",
    "    return x, rng_key\n",
    "\n",
    "def q_sample(x, t, rng_key):\n",
    "    t = jnp.full((x.shape[0], ), t)\n",
    "    rng_key, normal_key = jax.random.split(rng_key)\n",
    "    eps = jax.random.normal(normal_key, x.shape, dtype=jnp.float32)\n",
    "    x_t = scheduler.q_sample(x, t, eps)\n",
    "    return x_t, rng_key\n",
    "\n",
    "def noise_step(x, t, rng_key):\n",
    "    t = jnp.full((x.shape[0], ), t)\n",
    "    rng_key, normal_key = jax.random.split(rng_key)\n",
    "    x = scheduler.q_sample_step(x, t, jax.random.normal(normal_key, x.shape))\n",
    "    return x, rng_key\n",
    "\n",
    "def index_from_embedding(x):\n",
    "    # x: (B, Nres, Nemb)\n",
    "    protoken_indexes = \\\n",
    "        jnp.argmin(protoken_emb_distance_fn(x[..., None, :protoken_emb.shape[-1]], \n",
    "                                            protoken_emb[None, None, ...]), axis=-1)\n",
    "    ret = {'protoken_indexes': protoken_indexes}\n",
    "    if bool(infer_protuple):\n",
    "        aatype_indexes = \\\n",
    "            jnp.argmin(aatype_emb_distance_fn(x[..., None, protoken_emb.shape[-1]:], \n",
    "                                                aatype_emb[None, None, ...]), axis=-1)\n",
    "        ret.update({'aatype_indexes': aatype_indexes})\n",
    "        \n",
    "    return ret            \n",
    "    \n",
    "pjit_denoise_step = jax.pmap(jax.jit(partial(denoise_step, clamp_x0_fn=None)), axis_name=\"i\", \n",
    "                            in_axes=(0, 0, 0, None, 0, 0))\n",
    "pjit_denoise_step_clamped = jax.pmap(jax.jit(partial(denoise_step, clamp_x0_fn=clamp_x0_fn)), axis_name=\"i\", \n",
    "                            in_axes=(0, 0, 0, None, 0, 0))\n",
    "pjit_q_sample = jax.pmap(jax.jit(q_sample), axis_name=\"i\",\n",
    "                            in_axes=(0, None, 0))\n",
    "pjit_noise_step = jax.pmap(jax.jit(noise_step), axis_name=\"i\",\n",
    "                            in_axes=(0, None, 0))\n",
    "pjit_index_from_embedding = jax.pmap(jax.jit(index_from_embedding), axis_name=\"i\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.5 Define Functional Utils for RePaint Algorithm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Prepare main inference function with RePaint algorithms ([arXiv:2201.09865v4](https://arxiv.org/abs/2201.09865)). We define the following auxiliary functions and hyper-parameters: \n",
    "* `make_repaint_info`: functions to prepare repaint mask and repaint context used in repaint algorithm\n",
    "* `n_eq_steps`: number of equilibirum steps in predictor-corrector tricks ([ICLR2021](https://openreview.net/forum?id=PxTIG12RRHS)), in general, more `n_eq_steps` lead to higher sampling quality\n",
    "* `phasing_time`: we use phasing in diffusion inference, in first phase (large noise scale), we do not use clamping trick ([arXiv:2205.14217v1](https://arxiv.org/abs/2205.14217)), to encourage diversity; in second phase (small noise scale), we use clamping trick to ensure robustness"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_repaint_info(aatypes, protokens, aatype_context_ids, protoken_context_ids):\n",
    "    protoken_context = protoken_emb[protokens]\n",
    "    aatype_context = aatype_emb[aatypes]\n",
    "    assert len(protoken_context) == len(aatype_context), 'seq_len mismatch: {} != {}'.format(len(protoken_context), len(aatype_context))\n",
    "    seq_len = len(protoken_context)\n",
    "    \n",
    "    repaint_context = np.concatenate([protoken_context, aatype_context], axis=-1)\n",
    "    repaint_mask_aa = np.array([[0,]*DIM_EMB_PTK+[1,]*DIM_EMB_AA if i in aatype_context_ids \\\n",
    "                                  else [0,]*DIM_EMB for i in range(seq_len)], \n",
    "                                dtype=np.bool_)\n",
    "    repaint_mask_ptk = np.array([[1,]*DIM_EMB_PTK+[0,]*DIM_EMB_AA if i in protoken_context_ids \\\n",
    "                                  else [0,]*DIM_EMB for i in range(seq_len)], \n",
    "                                dtype=np.bool_)\n",
    "            \n",
    "    return repaint_context, np.logical_or(repaint_mask_aa, repaint_mask_ptk)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_eq_steps = 50 ### more n eq steps -> higher quality, in RePaint, we recommand more n eq steps\n",
    "phasing_time = 250 ### controls balance between diversity & quality, larger phasing time -> higer quality, lower diversity\n",
    "def run_infer(x, seq_mask, residue_index, rng_keys, \n",
    "              repaint_context=None, repaint_mask=None, repaint_time_steps=np.arange(num_diffusion_timesteps)):\n",
    "    for ti in tqdm(range(num_diffusion_timesteps)):\n",
    "        t = num_diffusion_timesteps - ti\n",
    "        for eq_step in range(n_eq_steps):\n",
    "            denoise_fn = pjit_denoise_step if t > phasing_time else pjit_denoise_step_clamped\n",
    "            x, rng_keys = denoise_fn(params, x, seq_mask, t, residue_index, rng_keys)\n",
    "            x, rng_keys = pjit_noise_step(x, t, rng_keys)\n",
    "            \n",
    "            if repaint_context is not None and t in repaint_time_steps:\n",
    "                repaint_context_ = repaint_context[..., t-1] if len(repaint_context.shape) > len(x.shape) \\\n",
    "                                    else repaint_context\n",
    "                repaint_mask_ = repaint_mask[..., t-1] if len(repaint_mask.shape) > len(x.shape) \\\n",
    "                                    else repaint_mask\n",
    "                \n",
    "                repaint_context_t, rng_keys = pjit_q_sample(repaint_context_, t, rng_keys)\n",
    "                x = repaint_mask_ * repaint_context_t + (1 - repaint_mask_) * x\n",
    "            \n",
    "        x, rng_keys = denoise_fn(params, x, seq_mask, t, residue_index, rng_keys)\n",
    "\n",
    "    ret = {'embedding': x, 'seq_mask': seq_mask, 'residue_index': residue_index}\n",
    "    ret.update(pjit_index_from_embedding(x))\n",
    "    \n",
    "    return ret"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Example Application 1: (Contextual) Inverse Folding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following codes provide examples of (contextual) inverse folding with PT-DiT."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1 Obtain ProTokens from Structrues (in .pdb format)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To utilize PT-DiT in (contextual) inverse folding, we first need to encode protein backbone structures into ProTokens. Here we use script `infer_batch.py`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### example: 8CYK\n",
    "\n",
    "### obatin ProTokens\n",
    "working_dir = '../'\n",
    "os.system(f'''export PYTHONPATH={working_dir}/PROTOKEN\n",
    "          python {working_dir}/PROTOKEN/scripts/infer_batch.py\\\n",
    "            --encoder_config {working_dir}/PROTOKEN/config/encoder.yaml\\\n",
    "            --decoder_config {working_dir}/PROTOKEN/config/decoder.yaml\\\n",
    "            --vq_config {working_dir}/PROTOKEN/config/vq.yaml\\\n",
    "            --pdb_dir_path {working_dir}/example_scripts/results/inverse_folding\\\n",
    "            --save_dir_path {working_dir}/example_scripts/results/inverse_folding\\\n",
    "            --load_ckpt_path {working_dir}/ckpts/protoken_params_100000.pkl''')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### load context\n",
    "with open('./results/inverse_folding/stage_1/generator_inputs/8CYK_B.pkl', 'rb') as f:\n",
    "    data_dict = pkl.load(f)\n",
    "\n",
    "seq_len = len(data_dict['protokens'])\n",
    "seq_mask = np.ones(seq_len, dtype=np.bool_)\n",
    "residue_index = np.arange(seq_len, dtype=np.int32)\n",
    "protoken_context = data_dict['protokens'].astype(np.int32)\n",
    "aatype_context = data_dict['aatype'].astype(np.int32)\n",
    "\n",
    "input_dict = {\n",
    "    'seq_mask': seq_mask, 'residue_index': residue_index,\n",
    "}\n",
    "\n",
    "for k, v in input_dict.items(): print(k, v.shape, v.dtype)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 Make RePaint Information"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `make_repaint_info` function defines the repainting context and mask for RePaint algorithms. For inverse folding tasks, all structural contexts (represented as ProTokens) remain fixed. Partial sequence contexts can be specified through the `aatype_context_resids` variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "protoken_context_resids = np.arange(seq_len)\n",
    "#### contextual inverse folding: put sequence context here\n",
    "aatype_context_resids = [] \n",
    "\n",
    "repaint_context, repaint_mask = make_repaint_info(aatype_context, protoken_context, aatype_context_resids, protoken_context_resids)\n",
    "\n",
    "repaint_dict = {\n",
    "    'repaint_context': repaint_context, 'repaint_mask': repaint_mask.astype(np.float32)\n",
    "}\n",
    "for k, v in repaint_dict.items(): print(k, v.shape, v.dtype)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3 Run Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### preprocessing inputs\n",
    "\n",
    "def reshape_tile_pad_x(x):\n",
    "    x_shape = x.shape\n",
    "    x = np.pad(x, ((0, NRES - x_shape[0]), ) + ((0,0),) * (len(x_shape) - 1))\n",
    "    x_shape = x.shape\n",
    "    \n",
    "    x = np.tile(x[None, ...], (BATCH_SIZE, ) + (1, ) * len(x_shape))\n",
    "    x = x.reshape(NDEVICES, NSAMPLE_PER_DEVICE, *x_shape)\n",
    "    return x\n",
    "\n",
    "input_dict = jax.tree.map(\n",
    "    lambda x: jnp.array(reshape_tile_pad_x(x)), input_dict\n",
    ")\n",
    "repaint_dict = jax.tree.map(\n",
    "    lambda x: jnp.array(reshape_tile_pad_x(x)), repaint_dict\n",
    ")\n",
    "\n",
    "init_key, rng_key = jax.random.split(rng_key)\n",
    "x = jax.random.normal(init_key, (NDEVICES, NSAMPLE_PER_DEVICE, NRES, DIM_EMB))\n",
    "input_dict['x'] = x\n",
    "\n",
    "rng_keys, rng_key = split_multiple_rng_keys(rng_key, NDEVICES)\n",
    "rng_keys = jnp.reshape(rng_keys, (NDEVICES, -1))\n",
    "\n",
    "for k, v in input_dict.items(): print(k, v.shape, v.dtype)\n",
    "for k, v in repaint_dict.items(): print(k, v.shape, v.dtype)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ret = run_infer(input_dict['x'], input_dict['seq_mask'], input_dict['residue_index'], rng_keys,\n",
    "                repaint_dict['repaint_context'], repaint_dict['repaint_mask'])\n",
    "\n",
    "ret = jax.tree_util.tree_map(lambda x: np.array(x.reshape(BATCH_SIZE, *x.shape[2:])).tolist(), ret)\n",
    "with open('results/inverse_folding/result.pkl', 'wb') as f:\n",
    "    pkl.dump(ret, f)\n",
    "    \n",
    "ret_ = flatten_list_of_dicts([ret])\n",
    "with open('results/inverse_folding/result_flatten.pkl', 'wb') as f:\n",
    "    pkl.dump(ret_, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.4 Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, r in enumerate(ret_):\n",
    "    protoken_idx = np.array(r['protoken_indexes'])[:seq_len]\n",
    "    aatype_idx = np.array(r['aatype_indexes'])[:seq_len]\n",
    "    print('seq{}: {}'.format(i, ''.join(aatype_index_to_resname(aatype_idx))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Example Application 2: Contextual Protein Design"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following codes provide examples of contextual protein design with PT-DiT. The algorithm co-designs structures and sequences while accommodating optional structural and sequence constraints as design contexts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1 Obtain ProTokens from Structrues (in .pdb format)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To utilize PT-DiT in (contextual) protein design, we first need to encode template protein backbone structures into ProTokens, which serve as context during inference. Here we use script `infer_batch.py`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### example: 5jxe_G_VH \n",
    "\n",
    "### obatin ProTokens\n",
    "working_dir = '../'\n",
    "os.system(f'''export PYTHONPATH={working_dir}/PROTOKEN\n",
    "          python {working_dir}/PROTOKEN/scripts/infer_batch.py\\\n",
    "            --encoder_config {working_dir}/PROTOKEN/config/encoder.yaml\\\n",
    "            --decoder_config {working_dir}/PROTOKEN/config/decoder.yaml\\\n",
    "            --vq_config {working_dir}/PROTOKEN/config/vq.yaml\\\n",
    "            --pdb_dir_path {working_dir}/example_scripts/results/contextual_scaffolding\\\n",
    "            --save_dir_path {working_dir}/example_scripts/results/contextual_scaffolding\\\n",
    "            --load_ckpt_path {working_dir}/ckpts/protoken_params_100000.pkl''')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### load context\n",
    "with open('./results/contextual_scaffolding/stage_1/generator_inputs/5jxe_G_VH.pkl', 'rb') as f:\n",
    "    data_dict = pkl.load(f)\n",
    "\n",
    "seq_len = len(data_dict['protokens'])\n",
    "seq_mask = np.ones(seq_len, dtype=np.bool_)\n",
    "residue_index = np.arange(seq_len, dtype=np.int32)\n",
    "protoken_context = data_dict['protokens'].astype(np.int32)\n",
    "aatype_context = data_dict['aatype'].astype(np.int32)\n",
    "\n",
    "input_dict = {\n",
    "    'seq_mask': seq_mask, 'residue_index': residue_index,\n",
    "}\n",
    "\n",
    "for k, v in input_dict.items(): print(k, v.shape, v.dtype)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2 Select CDR3 as context"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this demonstration, the CDR3 region of an antibody serves as the design context. We first parse structural annotations to identify CDR and FWR regions, then specifically extract CDR3 residues. \n",
    "\n",
    "Design constraints can be modified by adjusting the `aatype_context_resids` (sequence context) and `protoken_context_resids` (structural context) variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ast\n",
    "\n",
    "def parse_annotation(annotation_file, aatype_indexes):\n",
    "    seq_str = aatype_index_to_resname(aatype_indexes)\n",
    "    with open(annotation_file, 'r') as f:\n",
    "        contents = f.readlines()\n",
    "        annotation_dict = ast.literal_eval(contents[0])\n",
    "    annotation_resid_dict = {}\n",
    "    for k, v in annotation_dict.items():\n",
    "        start_id = seq_str.find(v)\n",
    "        if start_id == -1:\n",
    "            raise ValueError('can not find {} in {}'.format(v, seq_str))\n",
    "        end_id = start_id + len(v)\n",
    "        annotation_resid_dict[k] = np.arange(start_id, end_id)\n",
    "        \n",
    "    return annotation_resid_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "annotation_resid_dict = parse_annotation('./results/contextual_scaffolding/5jxe_G_VH_annotation.txt', \n",
    "                                         aatype_context)\n",
    "\n",
    "### select H-CDR3 as context\n",
    "protoken_context_resids = annotation_resid_dict['H-CDR3']\n",
    "aatype_context_resids = annotation_resid_dict['H-CDR3']\n",
    "\n",
    "repaint_context, repaint_mask = make_repaint_info(aatype_context, protoken_context, aatype_context_resids, protoken_context_resids)\n",
    "\n",
    "repaint_dict = {\n",
    "    'repaint_context': repaint_context, 'repaint_mask': repaint_mask.astype(np.float32)\n",
    "}\n",
    "for k, v in repaint_dict.items(): print(k, v.shape, v.dtype)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.3 Run Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### preprocessing inputs\n",
    "\n",
    "def reshape_tile_pad_x(x):\n",
    "    x_shape = x.shape\n",
    "    x = np.pad(x, ((0, NRES - x_shape[0]), ) + ((0,0),) * (len(x_shape) - 1))\n",
    "    x_shape = x.shape\n",
    "    \n",
    "    x = np.tile(x[None, ...], (BATCH_SIZE, ) + (1, ) * len(x_shape))\n",
    "    x = x.reshape(NDEVICES, NSAMPLE_PER_DEVICE, *x_shape)\n",
    "    return x\n",
    "\n",
    "input_dict = jax.tree.map(\n",
    "    lambda x: jnp.array(reshape_tile_pad_x(x)), input_dict\n",
    ")\n",
    "repaint_dict = jax.tree.map(\n",
    "    lambda x: jnp.array(reshape_tile_pad_x(x)), repaint_dict\n",
    ")\n",
    "\n",
    "init_key, rng_key = jax.random.split(rng_key)\n",
    "x = jax.random.normal(init_key, (NDEVICES, NSAMPLE_PER_DEVICE, NRES, DIM_EMB))\n",
    "input_dict['x'] = x\n",
    "\n",
    "rng_keys, rng_key = split_multiple_rng_keys(rng_key, NDEVICES)\n",
    "rng_keys = jnp.reshape(rng_keys, (NDEVICES, -1))\n",
    "\n",
    "for k, v in input_dict.items(): print(k, v.shape, v.dtype)\n",
    "for k, v in repaint_dict.items(): print(k, v.shape, v.dtype)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ret = run_infer(input_dict['x'], input_dict['seq_mask'], input_dict['residue_index'], rng_keys,\n",
    "                repaint_dict['repaint_context'], repaint_dict['repaint_mask'])\n",
    "\n",
    "ret = jax.tree_util.tree_map(lambda x: np.array(x.reshape(BATCH_SIZE, *x.shape[2:])).tolist(), ret)\n",
    "with open('results/contextual_scaffolding/result.pkl', 'wb') as f:\n",
    "    pkl.dump(ret, f)\n",
    "    \n",
    "ret_ = flatten_list_of_dicts([ret])\n",
    "with open('results/contextual_scaffolding/result_flatten.pkl', 'wb') as f:\n",
    "    pkl.dump(ret_, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.4 Decode Structures"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we decode 3D coordinates of protein structures from generated ProTokens (with `decode_structure.py` script), and save designed sequences & structures in .pdb format. pdb files are saved in `results/contextual_scaffolding/pdb`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "working_dir = '../'\n",
    "\n",
    "os.system(f'''export PYTHONPATH={working_dir}/PROTOKEN\n",
    "          python {working_dir}/PROTOKEN/scripts/decode_structure.py\\\n",
    "                --decoder_config {working_dir}/PROTOKEN/config/decoder.yaml\\\n",
    "                --vq_config {working_dir}/PROTOKEN/config/vq.yaml\\\n",
    "                --input_path results/contextual_scaffolding/result_flatten.pkl\\\n",
    "                --output_dir results/contextual_scaffolding/pdb\\\n",
    "                --load_ckpt_path {working_dir}/ckpts/protoken_params_100000.pkl\\\n",
    "                --padding_len {NRES}''')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
